version: "3.9"

services:
  llm-service:
    build: ./llm_service
    image: vllm-qwen3-ai:latest
    container_name: llm-service-qwen3
    ports:
      - "8005:8005"
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - TRUST_REMOTE_CODE=true
    volumes:
      - ./llm_service:/app
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model ./models
      --tokenizer ./models
      --port 8005
      --dtype float16
      --max-num-seqs 16
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    networks:
      - docker-network

  backend-service:
    build: ./backend
    image: vllm-qwen3-backend:latest
    container_name: backend-service-qwen3
    ports:
      - "8006:8006"
    depends_on:
      - llm-service
    networks:
      - docker-network

networks:
  docker-network:
    external: true
