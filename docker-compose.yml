version: "3.9"

services:
  llm-service:
    build: ./llm_service
    image: vllm-qwen3-ai:latest
    container_name: llm-service-qwen3
    ports:
      - "8005:8005"
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - TRUST_REMOTE_CODE=true
    volumes:
      - ./llm_service:/app
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model ./models
      --tokenizer ./models
      --port 8005
      --dtype float16
      --max-num-seqs 16
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    networks:
      - docker-network

  backend-service:
    build: ./backend
    image: vllm-qwen3-backend:latest
    container_name: backend-service-qwen3
    ports:
      - "8006:8006"
    depends_on:
      - llm-service
    networks:
      - docker-network
    environment:
      - LLM_URL=http://llm-service:8005/v1/chat/completions

  chainlit-ui:
    build:
      context: ./frontend
    image: vllm-qwen3-chainlit:latest
    container_name: frontend-service-qwen3
    ports:
      - "8007:8007"
    depends_on:
      - backend-service
    networks:
      - docker-network
    environment:
      - BACKEND_URL=http://backend-service:8006/qwen3/chat
    command: >
      chainlit run app.py --host 0.0.0.0 --port 8007

networks:
  docker-network:
    external: true
